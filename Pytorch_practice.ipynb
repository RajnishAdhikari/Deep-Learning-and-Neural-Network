{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, PyTorch is a library for processing tensors. A tensor is a number vector, matrix, or n-dimensional array. Let's create a tensor with a single numer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number\n",
    "t1 = torch.tensor(4.)\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. is a shorthand for 4.0. It is used to indicate to Python (and PyTorch) that you want to create a floating-point number. We can verify this by checking the dtype attribute of our tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try creating more complex tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating tensor from Vector\n",
    "t2 = torch.tensor([1., 2, 3, 4])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating tensor from Matrix\n",
    "t3 = torch.tensor([[5., 6], \n",
    "                   [7, 8], \n",
    "                   [9, 10]])\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11., 12., 13.],\n",
       "         [13., 14., 15.]],\n",
       "\n",
       "        [[15., 16., 17.],\n",
       "         [17., 18., 19.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-d arry as a tensor\n",
    "\n",
    "# 3-dimensional array\n",
    "t4 = torch.tensor([\n",
    "    [[11, 12, 13], \n",
    "     [13, 14, 15]], \n",
    "    [[15, 16, 17], \n",
    "     [17, 18, 19.]]])\n",
    "t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can have any number of dimensions and different lengths along each dimension. We can inspect the length along each dimension using the .shape property of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t1)\n",
    "t1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a null element as shape is []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t2)\n",
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  6.],\n",
      "        [ 7.,  8.],\n",
      "        [ 9., 10.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t3)\n",
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[11., 12., 13.],\n",
      "         [13., 14., 15.]],\n",
      "\n",
      "        [[15., 16., 17.],\n",
      "         [17., 18., 19.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t4)\n",
    "t4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it's not possible to create tensors with an improper shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\data science\\Deep Learning and Neural Network\\Pytorch_practice.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/data%20science/Deep%20Learning%20and%20Neural%20Network/Pytorch_practice.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Matrix \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/data%20science/Deep%20Learning%20and%20Neural%20Network/Pytorch_practice.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m t5 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([[\u001b[39m5.\u001b[39;49m, \u001b[39m6\u001b[39;49m, \u001b[39m11\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/data%20science/Deep%20Learning%20and%20Neural%20Network/Pytorch_practice.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                    [\u001b[39m7\u001b[39;49m,\u001b[39m8\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/data%20science/Deep%20Learning%20and%20Neural%20Network/Pytorch_practice.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                    [\u001b[39m9\u001b[39;49m,\u001b[39m10\u001b[39;49m]])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/data%20science/Deep%20Learning%20and%20Neural%20Network/Pytorch_practice.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m t5\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 2)"
     ]
    }
   ],
   "source": [
    "# Matrix \n",
    "t5 = torch.tensor([[5., 6, 11],\n",
    "                   [7,8],\n",
    "                   [9,10]])\n",
    "\n",
    "t5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ValueError is thrown because the lengths of the rows [5., 6, 11] and [7, 8] don't match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor operations and gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine tensors with the usual arithmetic operations. Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensors.\n",
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "\n",
    "# required_grad helps to calculate the gradient during backpropagagtion\n",
    "\n",
    "b = torch.tensor(5., requires_grad=True)\n",
    "x, w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created three tensors: x, w, and b, all numbers. w and b have an additional parameter requires_grad set to True. We'll see what it does in just a moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new tensor y by combining these tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arithmetic operations\n",
    "y = w * x + b\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, y is a tensor with the value 3 * 4 + 5 = 17. What makes PyTorch unique is that we can automatically compute the derivative of y w.r.t. the tensors that have requires_grad set to True i.e. w and b. This feature of PyTorch is called autograd (automatic gradients).\n",
    "\n",
    "To compute the derivatives, we can invoke the .backward method on our result y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derivatives \n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: None\n",
      "dy/dw: tensor(3.)\n",
      "dy/db: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Display gradients\n",
    "print('dy/dx:', x.grad)\n",
    "print('dy/dw:', w.grad)\n",
    "print('dy/db:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, dy/dw has the same value as x, i.e., 3, and dy/db has the value 1. Note that x.grad is None because x doesn't have requires_grad set to True.\n",
    "\n",
    "The \"grad\" in w.grad is short for gradient, which is another term for derivative. The term gradient is primarily used while dealing with vectors and matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from arithmetic operations, the torch module also contains many functions for creating and manipulating tensors. Let's look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42, 42],\n",
       "        [42, 42],\n",
       "        [42, 42]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor with a fixed value for every element\n",
    "t6 = torch.full((3, 2), 42)\n",
    "t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42, 42],\n",
       "        [42, 42],\n",
       "        [42, 42]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.],\n",
       "        [42., 42.],\n",
       "        [42., 42.],\n",
       "        [42., 42.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate two tensors with compatible shapes\n",
    "t7 = torch.cat((t3, t6))\n",
    "t7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9589, -0.2794],\n",
       "        [ 0.6570,  0.9894],\n",
       "        [ 0.4121, -0.5440],\n",
       "        [-0.9165, -0.9165],\n",
       "        [-0.9165, -0.9165],\n",
       "        [-0.9165, -0.9165]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the sin of each element\n",
    "t8 = torch.sin(t7)\n",
    "t8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9589, -0.2794],\n",
       "         [ 0.6570,  0.9894]],\n",
       "\n",
       "        [[ 0.4121, -0.5440],\n",
       "         [-0.9165, -0.9165]],\n",
       "\n",
       "        [[-0.9165, -0.9165],\n",
       "         [-0.9165, -0.9165]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the shape of a tensor   from 2d tensor to 3d tensor \n",
    "t9 = t8.reshape(3, 2, 2)\n",
    "t9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interoperability with Numpy\n",
    "\n",
    "Numpy is a popular open-source library used for mathematical and scientific computing in Python. It enables efficient operations on large multi-dimensional arrays and has a vast ecosystem of supporting libraries, including:\n",
    "\n",
    "* Pandas for file I/O and data analysis\n",
    "* Matplotlib for plotting and visualization\n",
    "* OpenCV for image and video processing\n",
    "\n",
    "\n",
    "Instead of reinventing the wheel, PyTorch interoperates well with Numpy to leverage its existing ecosystem of tools and libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we create an array in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2], [3, 4.]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a Numpy array to a PyTorch tensor using torch.from_numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the numpy array to a torch tensor.\n",
    "y = torch.from_numpy(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the numpy array and torch tensor have similar data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a PyTorch tensor to a Numpy array using the .numpy method of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a torch tensor to a numpy array\n",
    "z = y.numpy()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interoperability between PyTorch and Numpy is essential because most datasets you'll work with will likely be read and preprocessed as Numpy arrays.\n",
    "\n",
    "You might wonder why we need a library like PyTorch at all since Numpy already provides data structures and utilities for working with multi-dimensional numeric data. There are two main reasons:\n",
    "\n",
    "1. Autograd: The ability to automatically compute gradients for tensor operations is essential for training deep learning models.\n",
    "2. GPU support: While working with massive datasets and large models, PyTorch tensor operations can be performed efficiently using a Graphics Processing Unit (GPU). Computations that might typically take hours can be completed within minutes using GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression from scrach using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making training data \n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets (apples, oranges)\n",
    "target = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]]) \n",
      "\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "#Convert input and target to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "target = torch.from_numpy(target)\n",
    "\n",
    "print(inputs,\"\\n\")\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0344,  0.7317,  0.5768],\n",
      "        [-0.1247,  1.4045, -0.5144]], requires_grad=True)\n",
      "tensor([ 0.5147, -0.8908], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# weights and biases\n",
    "w = torch.randn(2,3 , requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "def model(x):\n",
    "  return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -1.1735,  61.9887],\n",
      "        [  7.6846,  78.4366],\n",
      "        [ 42.0190, 146.6284],\n",
      "        [-52.1923,  27.7503],\n",
      "        [ 39.7556,  89.3301]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "#actual\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function MSE\n",
    "def MSE(actual, target):\n",
    "  diff = actual - target\n",
    "  return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2579.9260, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# error\n",
    "loss = MSE(target, preds)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0344,  0.7317,  0.5768],\n",
      "        [-0.1247,  1.4045, -0.5144]], requires_grad=True) \n",
      "\n",
      "tensor([[-5894.8379, -5971.9126, -3757.5535],\n",
      "        [ -888.6219,  -788.4337,  -683.4446]])\n"
     ]
    }
   ],
   "source": [
    "print(w, \"\\n\")\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5147, -0.8908], requires_grad=True) \n",
      "\n",
      "tensor([-68.9813, -11.1732])\n"
     ]
    }
   ],
   "source": [
    "print(b, \"\\n\")\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "#reset grad\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -1.1735,  61.9887],\n",
      "        [  7.6846,  78.4366],\n",
      "        [ 42.0190, 146.6284],\n",
      "        [-52.1923,  27.7503],\n",
      "        [ 39.7556,  89.3301]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# adjust params\n",
    "\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2579.9260, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "loss = MSE(target, preds)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5894.8379, -5971.9126, -3757.5535],\n",
      "        [ -888.6219,  -788.4337,  -683.4446]]) \n",
      "\n",
      "tensor([-68.9813, -11.1732])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(w.grad, \"\\n\")\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # adjust weight & reset grad\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9755,  0.7914,  0.6143],\n",
      "        [-0.1158,  1.4124, -0.5075]], requires_grad=True)\n",
      "tensor([ 0.5154, -0.8907], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1793.0745, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# calculate again\n",
    "preds = model(inputs)\n",
    "loss = MSE(target, preds)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs(0/100) & Loss 1793.074462890625\n",
      "Epochs(1/100) & Loss 1262.53564453125\n",
      "Epochs(2/100) & Loss 904.724609375\n",
      "Epochs(3/100) & Loss 663.3155517578125\n",
      "Epochs(4/100) & Loss 500.3505859375\n",
      "Epochs(5/100) & Loss 390.2509765625\n",
      "Epochs(6/100) & Loss 315.7796630859375\n",
      "Epochs(7/100) & Loss 265.3199157714844\n",
      "Epochs(8/100) & Loss 231.0445098876953\n",
      "Epochs(9/100) & Loss 207.67788696289062\n",
      "Epochs(10/100) & Loss 191.66526794433594\n",
      "Epochs(11/100) & Loss 180.61123657226562\n",
      "Epochs(12/100) & Loss 172.90097045898438\n",
      "Epochs(13/100) & Loss 167.44692993164062\n",
      "Epochs(14/100) & Loss 163.51559448242188\n",
      "Epochs(15/100) & Loss 160.6131134033203\n",
      "Epochs(16/100) & Loss 158.4064483642578\n",
      "Epochs(17/100) & Loss 156.6708984375\n",
      "Epochs(18/100) & Loss 155.25546264648438\n",
      "Epochs(19/100) & Loss 154.0581512451172\n",
      "Epochs(20/100) & Loss 153.01004028320312\n",
      "Epochs(21/100) & Loss 152.0648956298828\n",
      "Epochs(22/100) & Loss 151.1914825439453\n",
      "Epochs(23/100) & Loss 150.36843872070312\n",
      "Epochs(24/100) & Loss 149.58206176757812\n",
      "Epochs(25/100) & Loss 148.8222198486328\n",
      "Epochs(26/100) & Loss 148.08267211914062\n",
      "Epochs(27/100) & Loss 147.35867309570312\n",
      "Epochs(28/100) & Loss 146.64776611328125\n",
      "Epochs(29/100) & Loss 145.94732666015625\n",
      "Epochs(30/100) & Loss 145.2564239501953\n",
      "Epochs(31/100) & Loss 144.57374572753906\n",
      "Epochs(32/100) & Loss 143.8987579345703\n",
      "Epochs(33/100) & Loss 143.23106384277344\n",
      "Epochs(34/100) & Loss 142.56991577148438\n",
      "Epochs(35/100) & Loss 141.91575622558594\n",
      "Epochs(36/100) & Loss 141.2677459716797\n",
      "Epochs(37/100) & Loss 140.62586975097656\n",
      "Epochs(38/100) & Loss 139.9902801513672\n",
      "Epochs(39/100) & Loss 139.3606414794922\n",
      "Epochs(40/100) & Loss 138.7367401123047\n",
      "Epochs(41/100) & Loss 138.11888122558594\n",
      "Epochs(42/100) & Loss 137.50668334960938\n",
      "Epochs(43/100) & Loss 136.89993286132812\n",
      "Epochs(44/100) & Loss 136.29881286621094\n",
      "Epochs(45/100) & Loss 135.70333862304688\n",
      "Epochs(46/100) & Loss 135.1132354736328\n",
      "Epochs(47/100) & Loss 134.52859497070312\n",
      "Epochs(48/100) & Loss 133.94921875\n",
      "Epochs(49/100) & Loss 133.37484741210938\n",
      "Epochs(50/100) & Loss 132.80587768554688\n",
      "Epochs(51/100) & Loss 132.24208068847656\n",
      "Epochs(52/100) & Loss 131.6832275390625\n",
      "Epochs(53/100) & Loss 131.12940979003906\n",
      "Epochs(54/100) & Loss 130.5804443359375\n",
      "Epochs(55/100) & Loss 130.0364532470703\n",
      "Epochs(56/100) & Loss 129.49728393554688\n",
      "Epochs(57/100) & Loss 128.96279907226562\n",
      "Epochs(58/100) & Loss 128.43313598632812\n",
      "Epochs(59/100) & Loss 127.90802001953125\n",
      "Epochs(60/100) & Loss 127.38752746582031\n",
      "Epochs(61/100) & Loss 126.87165832519531\n",
      "Epochs(62/100) & Loss 126.36009216308594\n",
      "Epochs(63/100) & Loss 125.85307312011719\n",
      "Epochs(64/100) & Loss 125.35060119628906\n",
      "Epochs(65/100) & Loss 124.8521499633789\n",
      "Epochs(66/100) & Loss 124.3582534790039\n",
      "Epochs(67/100) & Loss 123.868408203125\n",
      "Epochs(68/100) & Loss 123.38275146484375\n",
      "Epochs(69/100) & Loss 122.9012680053711\n",
      "Epochs(70/100) & Loss 122.42390441894531\n",
      "Epochs(71/100) & Loss 121.950439453125\n",
      "Epochs(72/100) & Loss 121.48123931884766\n",
      "Epochs(73/100) & Loss 121.0157470703125\n",
      "Epochs(74/100) & Loss 120.55418395996094\n",
      "Epochs(75/100) & Loss 120.09648132324219\n",
      "Epochs(76/100) & Loss 119.64261627197266\n",
      "Epochs(77/100) & Loss 119.19246673583984\n",
      "Epochs(78/100) & Loss 118.74609375\n",
      "Epochs(79/100) & Loss 118.30335998535156\n",
      "Epochs(80/100) & Loss 117.86417388916016\n",
      "Epochs(81/100) & Loss 117.42869567871094\n",
      "Epochs(82/100) & Loss 116.9968490600586\n",
      "Epochs(83/100) & Loss 116.56825256347656\n",
      "Epochs(84/100) & Loss 116.1433334350586\n",
      "Epochs(85/100) & Loss 115.72174072265625\n",
      "Epochs(86/100) & Loss 115.30350494384766\n",
      "Epochs(87/100) & Loss 114.88863372802734\n",
      "Epochs(88/100) & Loss 114.47718811035156\n",
      "Epochs(89/100) & Loss 114.06892395019531\n",
      "Epochs(90/100) & Loss 113.66389465332031\n",
      "Epochs(91/100) & Loss 113.2621078491211\n",
      "Epochs(92/100) & Loss 112.86329650878906\n",
      "Epochs(93/100) & Loss 112.4677734375\n",
      "Epochs(94/100) & Loss 112.07533264160156\n",
      "Epochs(95/100) & Loss 111.68587493896484\n",
      "Epochs(96/100) & Loss 111.2995834350586\n",
      "Epochs(97/100) & Loss 110.9161148071289\n",
      "Epochs(98/100) & Loss 110.53565979003906\n",
      "Epochs(99/100) & Loss 110.15799713134766\n",
      "Epochs(100/100) & Loss 109.7833480834961\n",
      "Epochs(101/100) & Loss 109.4114990234375\n",
      "Epochs(102/100) & Loss 109.04254150390625\n",
      "Epochs(103/100) & Loss 108.67622375488281\n",
      "Epochs(104/100) & Loss 108.31285095214844\n",
      "Epochs(105/100) & Loss 107.95204162597656\n",
      "Epochs(106/100) & Loss 107.59391021728516\n",
      "Epochs(107/100) & Loss 107.23856353759766\n",
      "Epochs(108/100) & Loss 106.88570404052734\n",
      "Epochs(109/100) & Loss 106.5355453491211\n",
      "Epochs(110/100) & Loss 106.18790435791016\n",
      "Epochs(111/100) & Loss 105.8427505493164\n",
      "Epochs(112/100) & Loss 105.50022888183594\n",
      "Epochs(113/100) & Loss 105.1600570678711\n",
      "Epochs(114/100) & Loss 104.82243347167969\n",
      "Epochs(115/100) & Loss 104.4872817993164\n",
      "Epochs(116/100) & Loss 104.15438079833984\n",
      "Epochs(117/100) & Loss 103.8240737915039\n",
      "Epochs(118/100) & Loss 103.49598693847656\n",
      "Epochs(119/100) & Loss 103.170166015625\n",
      "Epochs(120/100) & Loss 102.8466796875\n",
      "Epochs(121/100) & Loss 102.52547454833984\n",
      "Epochs(122/100) & Loss 102.20655822753906\n",
      "Epochs(123/100) & Loss 101.88981628417969\n",
      "Epochs(124/100) & Loss 101.5751953125\n",
      "Epochs(125/100) & Loss 101.26290130615234\n",
      "Epochs(126/100) & Loss 100.95255279541016\n",
      "Epochs(127/100) & Loss 100.64442443847656\n",
      "Epochs(128/100) & Loss 100.33844757080078\n",
      "Epochs(129/100) & Loss 100.03449249267578\n",
      "Epochs(130/100) & Loss 99.73271179199219\n",
      "Epochs(131/100) & Loss 99.43276977539062\n",
      "Epochs(132/100) & Loss 99.13502502441406\n",
      "Epochs(133/100) & Loss 98.83912658691406\n",
      "Epochs(134/100) & Loss 98.54515075683594\n",
      "Epochs(135/100) & Loss 98.25315856933594\n",
      "Epochs(136/100) & Loss 97.96315002441406\n",
      "Epochs(137/100) & Loss 97.67497253417969\n",
      "Epochs(138/100) & Loss 97.38873291015625\n",
      "Epochs(139/100) & Loss 97.10440063476562\n",
      "Epochs(140/100) & Loss 96.8218994140625\n",
      "Epochs(141/100) & Loss 96.5410385131836\n",
      "Epochs(142/100) & Loss 96.26219177246094\n",
      "Epochs(143/100) & Loss 95.98493957519531\n",
      "Epochs(144/100) & Loss 95.70967102050781\n",
      "Epochs(145/100) & Loss 95.43601989746094\n",
      "Epochs(146/100) & Loss 95.16413879394531\n",
      "Epochs(147/100) & Loss 94.89378356933594\n",
      "Epochs(148/100) & Loss 94.62532043457031\n",
      "Epochs(149/100) & Loss 94.3583984375\n",
      "Epochs(150/100) & Loss 94.09323120117188\n",
      "Epochs(151/100) & Loss 93.8295669555664\n",
      "Epochs(152/100) & Loss 93.56764221191406\n",
      "Epochs(153/100) & Loss 93.3072738647461\n",
      "Epochs(154/100) & Loss 93.04846954345703\n",
      "Epochs(155/100) & Loss 92.79122924804688\n",
      "Epochs(156/100) & Loss 92.53561401367188\n",
      "Epochs(157/100) & Loss 92.28142547607422\n",
      "Epochs(158/100) & Loss 92.02884674072266\n",
      "Epochs(159/100) & Loss 91.77762603759766\n",
      "Epochs(160/100) & Loss 91.52802276611328\n",
      "Epochs(161/100) & Loss 91.2797622680664\n",
      "Epochs(162/100) & Loss 91.0330810546875\n",
      "Epochs(163/100) & Loss 90.78777313232422\n",
      "Epochs(164/100) & Loss 90.54389953613281\n",
      "Epochs(165/100) & Loss 90.30140686035156\n",
      "Epochs(166/100) & Loss 90.06025695800781\n",
      "Epochs(167/100) & Loss 89.82069396972656\n",
      "Epochs(168/100) & Loss 89.58222198486328\n",
      "Epochs(169/100) & Loss 89.3453369140625\n",
      "Epochs(170/100) & Loss 89.10966491699219\n",
      "Epochs(171/100) & Loss 88.8753662109375\n",
      "Epochs(172/100) & Loss 88.64230346679688\n",
      "Epochs(173/100) & Loss 88.41059875488281\n",
      "Epochs(174/100) & Loss 88.18019104003906\n",
      "Epochs(175/100) & Loss 87.9510498046875\n",
      "Epochs(176/100) & Loss 87.72312927246094\n",
      "Epochs(177/100) & Loss 87.4964828491211\n",
      "Epochs(178/100) & Loss 87.27098846435547\n",
      "Epochs(179/100) & Loss 87.04671478271484\n",
      "Epochs(180/100) & Loss 86.82376098632812\n",
      "Epochs(181/100) & Loss 86.60187530517578\n",
      "Epochs(182/100) & Loss 86.38127899169922\n",
      "Epochs(183/100) & Loss 86.16170501708984\n",
      "Epochs(184/100) & Loss 85.94347381591797\n",
      "Epochs(185/100) & Loss 85.72636413574219\n",
      "Epochs(186/100) & Loss 85.51032257080078\n",
      "Epochs(187/100) & Loss 85.29542541503906\n",
      "Epochs(188/100) & Loss 85.08158874511719\n",
      "Epochs(189/100) & Loss 84.86891174316406\n",
      "Epochs(190/100) & Loss 84.65723419189453\n",
      "Epochs(191/100) & Loss 84.44668579101562\n",
      "Epochs(192/100) & Loss 84.23717498779297\n",
      "Epochs(193/100) & Loss 84.02894592285156\n",
      "Epochs(194/100) & Loss 83.82148742675781\n",
      "Epochs(195/100) & Loss 83.61520385742188\n",
      "Epochs(196/100) & Loss 83.40986633300781\n",
      "Epochs(197/100) & Loss 83.20552062988281\n",
      "Epochs(198/100) & Loss 83.00233459472656\n",
      "Epochs(199/100) & Loss 82.8000717163086\n",
      "Epochs(200/100) & Loss 82.59872436523438\n",
      "Epochs(201/100) & Loss 82.39845275878906\n",
      "Epochs(202/100) & Loss 82.1991958618164\n",
      "Epochs(203/100) & Loss 82.00077056884766\n",
      "Epochs(204/100) & Loss 81.80333709716797\n",
      "Epochs(205/100) & Loss 81.60682678222656\n",
      "Epochs(206/100) & Loss 81.41123962402344\n",
      "Epochs(207/100) & Loss 81.21658325195312\n",
      "Epochs(208/100) & Loss 81.02293395996094\n",
      "Epochs(209/100) & Loss 80.83011627197266\n",
      "Epochs(210/100) & Loss 80.63819122314453\n",
      "Epochs(211/100) & Loss 80.44715881347656\n",
      "Epochs(212/100) & Loss 80.25701141357422\n",
      "Epochs(213/100) & Loss 80.06767272949219\n",
      "Epochs(214/100) & Loss 79.87928771972656\n",
      "Epochs(215/100) & Loss 79.69181823730469\n",
      "Epochs(216/100) & Loss 79.50514221191406\n",
      "Epochs(217/100) & Loss 79.3193359375\n",
      "Epochs(218/100) & Loss 79.13424682617188\n",
      "Epochs(219/100) & Loss 78.95001983642578\n",
      "Epochs(220/100) & Loss 78.7665786743164\n",
      "Epochs(221/100) & Loss 78.58412170410156\n",
      "Epochs(222/100) & Loss 78.40225219726562\n",
      "Epochs(223/100) & Loss 78.22137451171875\n",
      "Epochs(224/100) & Loss 78.04109191894531\n",
      "Epochs(225/100) & Loss 77.8616943359375\n",
      "Epochs(226/100) & Loss 77.68302917480469\n",
      "Epochs(227/100) & Loss 77.5051040649414\n",
      "Epochs(228/100) & Loss 77.32804107666016\n",
      "Epochs(229/100) & Loss 77.15171813964844\n",
      "Epochs(230/100) & Loss 76.97606658935547\n",
      "Epochs(231/100) & Loss 76.80120086669922\n",
      "Epochs(232/100) & Loss 76.62708282470703\n",
      "Epochs(233/100) & Loss 76.4536361694336\n",
      "Epochs(234/100) & Loss 76.28092193603516\n",
      "Epochs(235/100) & Loss 76.10897827148438\n",
      "Epochs(236/100) & Loss 75.93766784667969\n",
      "Epochs(237/100) & Loss 75.76710510253906\n",
      "Epochs(238/100) & Loss 75.59730529785156\n",
      "Epochs(239/100) & Loss 75.42799377441406\n",
      "Epochs(240/100) & Loss 75.25952911376953\n",
      "Epochs(241/100) & Loss 75.09168243408203\n",
      "Epochs(242/100) & Loss 74.92449188232422\n",
      "Epochs(243/100) & Loss 74.75806427001953\n",
      "Epochs(244/100) & Loss 74.59214782714844\n",
      "Epochs(245/100) & Loss 74.42694854736328\n",
      "Epochs(246/100) & Loss 74.26246643066406\n",
      "Epochs(247/100) & Loss 74.0986328125\n",
      "Epochs(248/100) & Loss 73.93536376953125\n",
      "Epochs(249/100) & Loss 73.77275085449219\n",
      "Epochs(250/100) & Loss 73.61068725585938\n",
      "Epochs(251/100) & Loss 73.44933319091797\n",
      "Epochs(252/100) & Loss 73.28851318359375\n",
      "Epochs(253/100) & Loss 73.12843322753906\n",
      "Epochs(254/100) & Loss 72.96894836425781\n",
      "Epochs(255/100) & Loss 72.8099594116211\n",
      "Epochs(256/100) & Loss 72.65165710449219\n",
      "Epochs(257/100) & Loss 72.49385070800781\n",
      "Epochs(258/100) & Loss 72.33666229248047\n",
      "Epochs(259/100) & Loss 72.18014526367188\n",
      "Epochs(260/100) & Loss 72.02415466308594\n",
      "Epochs(261/100) & Loss 71.86872100830078\n",
      "Epochs(262/100) & Loss 71.71379852294922\n",
      "Epochs(263/100) & Loss 71.55950927734375\n",
      "Epochs(264/100) & Loss 71.40579223632812\n",
      "Epochs(265/100) & Loss 71.25259399414062\n",
      "Epochs(266/100) & Loss 71.09993743896484\n",
      "Epochs(267/100) & Loss 70.94783782958984\n",
      "Epochs(268/100) & Loss 70.79634094238281\n",
      "Epochs(269/100) & Loss 70.64534759521484\n",
      "Epochs(270/100) & Loss 70.49476623535156\n",
      "Epochs(271/100) & Loss 70.34480285644531\n",
      "Epochs(272/100) & Loss 70.19538116455078\n",
      "Epochs(273/100) & Loss 70.04655456542969\n",
      "Epochs(274/100) & Loss 69.89804077148438\n",
      "Epochs(275/100) & Loss 69.75019836425781\n",
      "Epochs(276/100) & Loss 69.60285186767578\n",
      "Epochs(277/100) & Loss 69.45599365234375\n",
      "Epochs(278/100) & Loss 69.30963897705078\n",
      "Epochs(279/100) & Loss 69.1636962890625\n",
      "Epochs(280/100) & Loss 69.01835632324219\n",
      "Epochs(281/100) & Loss 68.87342834472656\n",
      "Epochs(282/100) & Loss 68.72904205322266\n",
      "Epochs(283/100) & Loss 68.58509826660156\n",
      "Epochs(284/100) & Loss 68.44170379638672\n",
      "Epochs(285/100) & Loss 68.29875946044922\n",
      "Epochs(286/100) & Loss 68.15621948242188\n",
      "Epochs(287/100) & Loss 68.01417541503906\n",
      "Epochs(288/100) & Loss 67.87263488769531\n",
      "Epochs(289/100) & Loss 67.73152923583984\n",
      "Epochs(290/100) & Loss 67.59078979492188\n",
      "Epochs(291/100) & Loss 67.45075225830078\n",
      "Epochs(292/100) & Loss 67.3109359741211\n",
      "Epochs(293/100) & Loss 67.17160034179688\n",
      "Epochs(294/100) & Loss 67.03275299072266\n",
      "Epochs(295/100) & Loss 66.89440155029297\n",
      "Epochs(296/100) & Loss 66.7564697265625\n",
      "Epochs(297/100) & Loss 66.61883544921875\n",
      "Epochs(298/100) & Loss 66.48179626464844\n",
      "Epochs(299/100) & Loss 66.34515380859375\n",
      "Epochs(300/100) & Loss 66.2088394165039\n",
      "Epochs(301/100) & Loss 66.07305908203125\n",
      "Epochs(302/100) & Loss 65.93766784667969\n",
      "Epochs(303/100) & Loss 65.8027114868164\n",
      "Epochs(304/100) & Loss 65.66801452636719\n",
      "Epochs(305/100) & Loss 65.533935546875\n",
      "Epochs(306/100) & Loss 65.40022277832031\n",
      "Epochs(307/100) & Loss 65.26680755615234\n",
      "Epochs(308/100) & Loss 65.13389587402344\n",
      "Epochs(309/100) & Loss 65.00142669677734\n",
      "Epochs(310/100) & Loss 64.86921691894531\n",
      "Epochs(311/100) & Loss 64.73748016357422\n",
      "Epochs(312/100) & Loss 64.60609436035156\n",
      "Epochs(313/100) & Loss 64.47521209716797\n",
      "Epochs(314/100) & Loss 64.34464263916016\n",
      "Epochs(315/100) & Loss 64.21440887451172\n",
      "Epochs(316/100) & Loss 64.08462524414062\n",
      "Epochs(317/100) & Loss 63.955223083496094\n",
      "Epochs(318/100) & Loss 63.826072692871094\n",
      "Epochs(319/100) & Loss 63.69746017456055\n",
      "Epochs(320/100) & Loss 63.5692024230957\n",
      "Epochs(321/100) & Loss 63.44118118286133\n",
      "Epochs(322/100) & Loss 63.313720703125\n",
      "Epochs(323/100) & Loss 63.18648147583008\n",
      "Epochs(324/100) & Loss 63.05956268310547\n",
      "Epochs(325/100) & Loss 62.93328857421875\n",
      "Epochs(326/100) & Loss 62.80705642700195\n",
      "Epochs(327/100) & Loss 62.681304931640625\n",
      "Epochs(328/100) & Loss 62.555877685546875\n",
      "Epochs(329/100) & Loss 62.4308967590332\n",
      "Epochs(330/100) & Loss 62.30615997314453\n",
      "Epochs(331/100) & Loss 62.181861877441406\n",
      "Epochs(332/100) & Loss 62.05781936645508\n",
      "Epochs(333/100) & Loss 61.93415451049805\n",
      "Epochs(334/100) & Loss 61.8109016418457\n",
      "Epochs(335/100) & Loss 61.68791961669922\n",
      "Epochs(336/100) & Loss 61.565208435058594\n",
      "Epochs(337/100) & Loss 61.44293212890625\n",
      "Epochs(338/100) & Loss 61.32097244262695\n",
      "Epochs(339/100) & Loss 61.19928741455078\n",
      "Epochs(340/100) & Loss 61.078041076660156\n",
      "Epochs(341/100) & Loss 60.95705032348633\n",
      "Epochs(342/100) & Loss 60.836395263671875\n",
      "Epochs(343/100) & Loss 60.7160758972168\n",
      "Epochs(344/100) & Loss 60.59607696533203\n",
      "Epochs(345/100) & Loss 60.47636795043945\n",
      "Epochs(346/100) & Loss 60.35700607299805\n",
      "Epochs(347/100) & Loss 60.237998962402344\n",
      "Epochs(348/100) & Loss 60.1192512512207\n",
      "Epochs(349/100) & Loss 60.0008430480957\n",
      "Epochs(350/100) & Loss 59.88264083862305\n",
      "Epochs(351/100) & Loss 59.76484298706055\n",
      "Epochs(352/100) & Loss 59.64739227294922\n",
      "Epochs(353/100) & Loss 59.53020095825195\n",
      "Epochs(354/100) & Loss 59.41333770751953\n",
      "Epochs(355/100) & Loss 59.296791076660156\n",
      "Epochs(356/100) & Loss 59.180458068847656\n",
      "Epochs(357/100) & Loss 59.0644645690918\n",
      "Epochs(358/100) & Loss 58.94886016845703\n",
      "Epochs(359/100) & Loss 58.833412170410156\n",
      "Epochs(360/100) & Loss 58.71833419799805\n",
      "Epochs(361/100) & Loss 58.60357666015625\n",
      "Epochs(362/100) & Loss 58.48906326293945\n",
      "Epochs(363/100) & Loss 58.37485885620117\n",
      "Epochs(364/100) & Loss 58.26097869873047\n",
      "Epochs(365/100) & Loss 58.14726638793945\n",
      "Epochs(366/100) & Loss 58.0339469909668\n",
      "Epochs(367/100) & Loss 57.92094039916992\n",
      "Epochs(368/100) & Loss 57.80810546875\n",
      "Epochs(369/100) & Loss 57.69572067260742\n",
      "Epochs(370/100) & Loss 57.58342361450195\n",
      "Epochs(371/100) & Loss 57.47147750854492\n",
      "Epochs(372/100) & Loss 57.35984420776367\n",
      "Epochs(373/100) & Loss 57.248443603515625\n",
      "Epochs(374/100) & Loss 57.13739776611328\n",
      "Epochs(375/100) & Loss 57.02653884887695\n",
      "Epochs(376/100) & Loss 56.91594314575195\n",
      "Epochs(377/100) & Loss 56.80570602416992\n",
      "Epochs(378/100) & Loss 56.69560623168945\n",
      "Epochs(379/100) & Loss 56.58589553833008\n",
      "Epochs(380/100) & Loss 56.476478576660156\n",
      "Epochs(381/100) & Loss 56.367156982421875\n",
      "Epochs(382/100) & Loss 56.25818634033203\n",
      "Epochs(383/100) & Loss 56.14958572387695\n",
      "Epochs(384/100) & Loss 56.04108428955078\n",
      "Epochs(385/100) & Loss 55.932891845703125\n",
      "Epochs(386/100) & Loss 55.82494354248047\n",
      "Epochs(387/100) & Loss 55.717308044433594\n",
      "Epochs(388/100) & Loss 55.609954833984375\n",
      "Epochs(389/100) & Loss 55.502784729003906\n",
      "Epochs(390/100) & Loss 55.395957946777344\n",
      "Epochs(391/100) & Loss 55.2893180847168\n",
      "Epochs(392/100) & Loss 55.18284225463867\n",
      "Epochs(393/100) & Loss 55.07672119140625\n",
      "Epochs(394/100) & Loss 54.970916748046875\n",
      "Epochs(395/100) & Loss 54.865234375\n",
      "Epochs(396/100) & Loss 54.759864807128906\n",
      "Epochs(397/100) & Loss 54.65476608276367\n",
      "Epochs(398/100) & Loss 54.549827575683594\n",
      "Epochs(399/100) & Loss 54.44519805908203\n"
     ]
    }
   ],
   "source": [
    "# Training for multiple epochs\n",
    "for i in range(400):\n",
    "  preds = model(inputs)\n",
    "  loss = MSE(target, preds)\n",
    "  loss.backward()\n",
    "\n",
    "  with torch.no_grad():\n",
    "     w -= w.grad * 1e-5 # learning rate\n",
    "     b -= b.grad * 1e-5\n",
    "     w.grad.zero_()\n",
    "     b.grad.zero_()\n",
    "  print(f\"Epochs({i}/{100}) & Loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(54.3408, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = MSE(target, preds)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.3716231551738005"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "sqrt(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.9662,  70.9309],\n",
       "        [ 82.3482,  93.3911],\n",
       "        [118.6931, 148.5109],\n",
       "        [ 19.4476,  40.7168],\n",
       "        [103.2223, 104.1395]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can see they are almost close each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
